# Table of contents

* [âœˆï¸ Introduction](README.md)
* [ğŸŸ¢ Why DL is getting popular?](why-dl-is-getting-popular.md)
* [ğŸŸ¢ Perceptron Intuition](perceptron-intuition.md)
* [ğŸŸ¢ Usage of Perceptron](usage-of-perceptron.md)
* [ğŸŸ¢ ANN Intuition and Learning](ann-intuition-and-learning.md)
* [ğŸŸ¢ Slope](slope.md)
* [ğŸŸ¢ Back Propagation and Weight updation](back-propagation-and-weight-updation.md)
* [ğŸ”´ Chain rule of Derivatives](chain-rule-of-derivatives.md)
* [ğŸ”´ Vanishing Gradient Problem and Sigmoid](vanishing-gradient-problem-and-sigmoid.md)
* [ğŸŸ¢ Sigmoid Activation](sigmoid-activation.md)
* [ğŸŸ¢ Tanh Activation](tanh-activation.md)
* [ğŸ ---------- 10 ----------](10.md)
* [ğŸŸ¢ Relu](relu.md)
* [ğŸŸ¢ Leaky Relu and Parametric Relu](leaky-relu-and-parametric-relu.md)
* [ğŸŸ¢ Exponential Relu](exponential-relu.md)
* [âš”ï¸ Activation Functions](activation-functions.md)
* [ğŸŸ¢ Softmax for Multiclass Classification](softmax-for-multiclass-classification.md)
* [ğŸŸ¢ Which activation function to use when](which-activation-function-to-use-when.md)
* [ğŸŸ¢ Loss Function vs Cost Function](loss-function-vs-cost-function.md)
* [ğŸŸ¢ Mean Squared Error](mean-squared-error.md)
* [ğŸŸ¢ Mean Absolute Error](mean-absolute-error.md)
* [ğŸŸ¢ Huber Loss](huber-loss.md)
* [ğŸŸ¢ RMSE](rmse.md)
* [---------- 20 ----------](20.md)
* [ğŸŸ¢ Classification Cost Function](classification-cost-function.md)
* [ğŸŸ¢ Binary cross entropy](binary-cross-entropy.md)
* [Categorical Cross Entropy](categorical-cross-entropy.md)
* [ğŸŸ¢ Categorical Cross Entropy](categorical-cross-entropy-1.md)
* [Sparse categorical cross entropy](sparse-categorical-cross-entropy.md)
* [ğŸŸ¢ Sparse categorical cross entropy](sparse-categorical-cross-entropy-1.md)
* [ğŸŸ¢ Which Loss function to use when](which-loss-function-to-use-when.md)
* [ğŸŸ¢ Gradient Descent Optimizers](gradient-descent-optimizers.md)
* [ğŸŸ¢ Why is gradient descent said to be cpu intensive](why-is-gradient-descent-said-to-be-cpu-intensive.md)
* [ğŸŸ¢ SGD](sgd.md)
* [ğŸŸ¢ Mini Batch with SGD](mini-batch-with-sgd.md)
* [ğŸŸ¢ Exponential weighted average](exponential-weighted-average.md)
* [ğŸ ---------- 30 ----------](30.md)
* [ğŸŸ¢ SGD with momentum](sgd-with-momentum.md)
* [âœˆï¸ Adagrad](adagrad.md)
* [ğŸŸ¢ Adagrad](adagrad-1.md)
* [ğŸŸ¢ RMS Prop](rms-prop.md)
* [ğŸŸ¢ Adadelta](adadelta.md)
* [ğŸ”´ Adam](adam.md)
* [ğŸŸ¢ Exploding Gradient Problem](exploding-gradient-problem.md)
* [ğŸŸ¢ Key points in weight initialization](key-points-in-weight-initialization.md)
* [ğŸŸ¢ Input / Output](input-output.md)
* [ğŸŸ¢ Uniform Distribution](uniform-distribution.md)
* [ğŸŸ¢ Xavier/Glorot Initialization](xavier-glorot-initialization.md)
* [ğŸ ---------- 40 ----------](40.md)
* [ğŸŸ¢ Kaiming He Initialization](kaiming-he-initialization.md)
* [ğŸŸ¢ Dropout Layers](dropout-layers.md)
* [â„¹ï¸ ANN Project Implementation](ann-project-implementation.md)
* [ğŸŸ¢ Classification Problem Statement](classification-problem-statement.md)
* [ğŸŸ¢ Feature Transformation](feature-transformation.md)
* [ğŸŸ¢ Step by Step training with ANN](step-by-step-training-with-ann.md)
* [ğŸŸ¢ Predictions](predictions.md)
* [âœˆï¸ Streamlit App](streamlit-app.md)
* [ğŸŸ¢ ANN Regression Problem Statement](ann-regression-problem-statement.md)
* [ğŸŸ¢ Finding Optimal Layers and Neurons](finding-optimal-layers-and-neurons.md)
