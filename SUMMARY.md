# Table of contents

* [✈️ Introduction](README.md)
* [🟢 Why DL is getting popular?](why-dl-is-getting-popular.md)
* [🟢 Perceptron Intuition](perceptron-intuition.md)
* [🟢 Usage of Perceptron](usage-of-perceptron.md)
* [🟢 ANN Intuition and Learning](ann-intuition-and-learning.md)
* [🟢 Slope](slope.md)
* [🟢 Back Propagation and Weight updation](back-propagation-and-weight-updation.md)
* [🔴 Chain rule of Derivatives](chain-rule-of-derivatives.md)
* [🔴 Vanishing Gradient Problem and Sigmoid](vanishing-gradient-problem-and-sigmoid.md)
* [🟢 Sigmoid Activation](sigmoid-activation.md)
* [🟢 Tanh Activation](tanh-activation.md)
* [🏁 ---------- 10 ----------](10.md)
* [🟢 Relu](relu.md)
* [🟢 Leaky Relu and Parametric Relu](leaky-relu-and-parametric-relu.md)
* [🟢 Exponential Relu](exponential-relu.md)
* [⚔️ Activation Functions](activation-functions.md)
* [🟢 Softmax for Multiclass Classification](softmax-for-multiclass-classification.md)
* [🟢 Which activation function to use when](which-activation-function-to-use-when.md)
* [🟢 Loss Function vs Cost Function](loss-function-vs-cost-function.md)
* [🟢 Mean Squared Error](mean-squared-error.md)
* [🟢 Mean Absolute Error](mean-absolute-error.md)
* [🟢 Huber Loss](huber-loss.md)
* [🟢 RMSE](rmse.md)
* [---------- 20 ----------](20.md)
* [🟢 Classification Cost Function](classification-cost-function.md)
* [🟢 Binary cross entropy](binary-cross-entropy.md)
* [Categorical Cross Entropy](categorical-cross-entropy.md)
* [🟢 Categorical Cross Entropy](categorical-cross-entropy-1.md)
* [Sparse categorical cross entropy](sparse-categorical-cross-entropy.md)
* [🟢 Sparse categorical cross entropy](sparse-categorical-cross-entropy-1.md)
* [🟢 Which Loss function to use when](which-loss-function-to-use-when.md)
* [🟢 Gradient Descent Optimizers](gradient-descent-optimizers.md)
* [🟢 Why is gradient descent said to be cpu intensive](why-is-gradient-descent-said-to-be-cpu-intensive.md)
* [🟢 SGD](sgd.md)
* [🟢 Mini Batch with SGD](mini-batch-with-sgd.md)
* [🟢 Exponential weighted average](exponential-weighted-average.md)
* [🏁 ---------- 30 ----------](30.md)
* [🟢 SGD with momentum](sgd-with-momentum.md)
* [✈️ Adagrad](adagrad.md)
* [🟢 Adagrad](adagrad-1.md)
* [🟢 RMS Prop](rms-prop.md)
* [🟢 Adadelta](adadelta.md)
* [🔴 Adam](adam.md)
* [🟢 Exploding Gradient Problem](exploding-gradient-problem.md)
* [🟢 Key points in weight initialization](key-points-in-weight-initialization.md)
* [🟢 Input / Output](input-output.md)
* [🟢 Uniform Distribution](uniform-distribution.md)
* [🟢 Xavier/Glorot Initialization](xavier-glorot-initialization.md)
* [🏁 ---------- 40 ----------](40.md)
* [🟢 Kaiming He Initialization](kaiming-he-initialization.md)
* [🟢 Dropout Layers](dropout-layers.md)
* [ℹ️ ANN Project Implementation](ann-project-implementation.md)
* [🟢 Classification Problem Statement](classification-problem-statement.md)
* [🟢 Feature Transformation](feature-transformation.md)
* [🟢 Step by Step training with ANN](step-by-step-training-with-ann.md)
* [🟢 Predictions](predictions.md)
* [✈️ Streamlit App](streamlit-app.md)
* [🟢 ANN Regression Problem Statement](ann-regression-problem-statement.md)
* [🟢 Finding Optimal Layers and Neurons](finding-optimal-layers-and-neurons.md)
