# Table of contents

* [âœˆï¸ Introduction](README.md)
* [ğŸŸ¢ Why DL is getting popular?](why-dl-is-getting-popular.md)
* [ğŸŸ¢ Perceptron Intuition](perceptron-intuition.md)
* [ğŸŸ¢ Usage of Perceptron](usage-of-perceptron.md)
* [ğŸŸ¢ ANN Intuition and Learning](ann-intuition-and-learning.md)
* [ğŸŸ¢ Slope](slope.md)
* [ğŸŸ¢ Back Propagation and Weight updation](back-propagation-and-weight-updation.md)
* [ğŸ”´ Chain rule of Derivatives](chain-rule-of-derivatives.md)
* [ğŸ”´ Vanishing Gradient Problem and Sigmoid](vanishing-gradient-problem-and-sigmoid.md)
* [ğŸŸ¢ Sigmoid Activation](sigmoid-activation.md)
* [ğŸŸ¢ Tanh Activation](tanh-activation.md)
* [---------- 10 ----------](10.md)
* [Relu](relu.md)
* [Leaky Relu and Parametric Relu](leaky-relu-and-parametric-relu.md)
* [Exponential Relu](exponential-relu.md)
* [Softmax for Multiclass Classification](softmax-for-multiclass-classification.md)
* [Which activation function to use when](which-activation-function-to-use-when.md)
* [Loss Function vs Cost Function](loss-function-vs-cost-function.md)
* [Regression Cost Function](regression-cost-function.md)
* [Classification Cost Function](classification-cost-function.md)
* [Which Loss function to use when](which-loss-function-to-use-when.md)
* [Gradient Descent Optimizers](gradient-descent-optimizers.md)
* [SGD](sgd.md)
* [Mini Batch with SGD](mini-batch-with-sgd.md)
* [SGD with momentum](sgd-with-momentum.md)
* [Adagrad](adagrad.md)
* [Adadelta and RMS Prop](adadelta-and-rms-prop.md)
* [Adam](adam.md)
* [â„¹ï¸ ANN Project Implementation](ann-project-implementation.md)
* [Classification Problem Statement](classification-problem-statement.md)
* [Feature Transformation](feature-transformation.md)
* [Step by Step training with ANN](step-by-step-training-with-ann.md)
* [Predictions](predictions.md)
* [Streamlit App](streamlit-app.md)
* [ANN Regression Problem Statement](ann-regression-problem-statement.md)
* [Finding Optimal Layers and Neurons](finding-optimal-layers-and-neurons.md)
