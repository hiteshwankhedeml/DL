# ðŸŸ¢ Which activation function to use when



* If we use sigmoid/tanh in hidden layer then we get vanishing gradient problem
* <mark style="color:purple;background-color:purple;">**We use relu and its variants in hidden layer**</mark>
*   <mark style="color:purple;background-color:purple;">**In output we use sigmoid OR softmax**</mark>

    <figure><img src=".gitbook/assets/image (4) (1).png" alt=""><figcaption></figcaption></figure>
